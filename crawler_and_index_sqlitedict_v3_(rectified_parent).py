# -*- coding: utf-8 -*-
"""crawler and index_sqlitedict_v3 (rectified parent).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Cgy_KmYpx7iVwJp4L-5CBLdCpP8DU2T
"""


# # install necessary libraries
# !pip install bs4 sqlitedict nltk requests

"""# Crawler and indexer"""

# # set directory for database storage
# import os
# os.chdir('/content/drive/MyDrive/BDT/CSIT Search Engine')

# import necessary libraries
import sys
import re
from bs4 import BeautifulSoup
from sqlitedict import SqliteDict
import requests

from string import punctuation
import nltk
from nltk import word_tokenize, pos_tag
from nltk.stem import PorterStemmer
ps = PorterStemmer()
nltk.download('punkt')

## load stopwords
stopword_file = open("./stopwords.txt")
stopwords = set([rows.rstrip('\n') for rows in stopword_file])
print("stopwords:", stopwords)  # for checking

from datetime import datetime
from urllib.parse import urljoin
import collections


# To create queue for pages pending for crawling
queue = collections.deque()

# set starting page to be crawled
url = "https://www.cse.ust.hk/~kwtleung/COMP4321/testpage.htm"


"create mapping dictionaries"
# url to pageID
url_to_pageID = collections.OrderedDict()
# pageID to url
pageID_to_url = collections.OrderedDict()

# word to wordID in page body
body_word_to_wordID = collections.OrderedDict()
# wordID to word in page body
body_wordID_to_word = collections.OrderedDict()
# pageID to (wordID, counts) in page body
body_forward_index = collections.OrderedDict()
# wordID to (pageID, freq, tfidf) in page body
body_inverted_index = collections.OrderedDict()

# word to wordID in page title
title_word_to_wordID = collections.OrderedDict()
# word to wordID in page title
title_wordID_to_word = collections.OrderedDict()
# pageID to wordID in page title
title_forward_index = collections.OrderedDict()
# wordID to (pageID, tfidf) in page title
title_inverted_index = collections.OrderedDict()

# PageID to details of webpage (page title, last modified date, size of page, word freq dictionary and child page)
pageID_details = collections.OrderedDict()

# PageID to parent pageID
pageID_to_parentID = collections.OrderedDict()


def WordFreqCount(tokens):
    word_freq = collections.OrderedDict()
    for token in tokens:
        if token not in word_freq:
            word_freq[token] = 1
        else:
            word_freq[token] = word_freq[token] + 1
    return word_freq


def UpdateBodyWordIndex(word_freq):
    for word in word_freq.keys():
        if word not in body_word_to_wordID:
            current_WordID = len(body_word_to_wordID)
            body_wordID_to_word[current_WordID] = word
            body_word_to_wordID[word] = current_WordID
            
def  UpdateBodyInvertedIndex(word_freq, pageID):
    for word, freq in word_freq.items():
        wordID = body_word_to_wordID[word]
        if wordID not in body_inverted_index:
            body_inverted_index[wordID] = [[pageID, freq, 0]]
        else:
            body_inverted_index[wordID].append([pageID, freq, 0])


def UpdateTitleWordIndex(title_tokens):
    for title_word in title_tokens:
        if title_word not in title_word_to_wordID:
            current_title_wordID = len(title_word_to_wordID)
            title_wordID_to_word[current_title_wordID] = title_word
            title_word_to_wordID[title_word] = current_title_wordID
            
def UpdateTitleInvertedIndex(title_tokens, pageID):
    for title_token in title_tokens:
        wordID = title_word_to_wordID[title_token]
        if wordID not in title_inverted_index:
            title_inverted_index[wordID] = [[pageID, 0]]
        else:
            title_inverted_index[wordID].append([pageID, 0])


def tokenize_clean_stem(document):
    tokens = word_tokenize(document) # tokenization
    punc_table = str.maketrans('', '', punctuation)    # remove punctuation
    tokens = [w.translate(punc_table) for w in tokens]
    tokens = [word.lower() for word in tokens] # convert to lower case
    tokens = [re.sub('[^A-Za-z0-9]+', '', w) for w in tokens] 
    tokens = [w for w in tokens if w != ''] 
    stop_words = stopwords
    tokens = [w for w in tokens if w not in stop_words] # remove stopwords
    tokens = [ps.stem(w) for w in tokens] # stemming using porter algorithm with nltk library
    return tokens


def tokenize_bigram(text):
    dummy_tokens = text.copy()
    bi_list = list(nltk.bigrams(dummy_tokens))
    bigram_string = [bi[0] + " " + bi[1] for bi in bi_list]
    return bigram_string


save_html = True
def SaveHTML(pageID, html):
    html_save = open('./data/' + "page" + str(pageID) + ".html", "w")
    content = str(html).replace('\n', '<br>')
    html_save.write(str(content.encode('ascii', 'ignore')))
    html_save.close()


def crawl(url, parent_ID: list):
        url_full = requests.get(url)
        soup = BeautifulSoup(url_full.text, 'html.parser') 
        page_title = soup.find("title").text
        
        last_modified_date = datetime.now().strftime("%Y-%m-%d")
        # check if modified data is available in html
        try:
            existing_date = soup.find('span', {"class": "pull-right"})
            last_modified_date = re.findall(re.compile("\d+-\d+-\d+"), existing_date.text)[0]
        except AttributeError:
            pass

        currentPageID = len(url_to_pageID)
        # Check whether a page has been modified
        if url in url_to_pageID.keys():
            modify_date = datetime.strptime(pageID_details[url_to_pageID[url]][1], "%Y-%m-%d")
            if modify_date >= datetime.strptime(last_modified_date, "%Y-%m-%d"):
                raise Exception('Note: this page has not been modified')
            currentPageID = url_to_pageID[url]
        print(url)
        print(currentPageID)

        # url to pageID
        url_to_pageID[url] = currentPageID     

        # pageID to url
        pageID_to_url[currentPageID] = url


        # word to wordID in page body
        tokens_clean = tokenize_clean_stem(soup.text)     
        word_freq_uni = WordFreqCount(tokens_clean)
        word_freq_bi = WordFreqCount(tokenize_bigram(tokens_clean))
        word_freq_combine = dict(word_freq_uni)
        word_freq_combine.update(word_freq_bi)
        UpdateBodyWordIndex(word_freq_combine)

        # Update forward index in page body
        word_freq_id = collections.OrderedDict()
        for word, freq in word_freq_combine.items():
            wordID = body_word_to_wordID[word]
            word_freq_id[wordID] = freq
        body_forward_index[currentPageID] = word_freq_id
        
        # Update inverted index in page body
        UpdateBodyInvertedIndex(word_freq_combine, currentPageID)


        # word to wordID in page title
        title_tokens_clean = tokenize_clean_stem(page_title)
        UpdateTitleWordIndex(title_tokens_clean)
        UpdateTitleWordIndex(tokenize_bigram(title_tokens_clean))
        
        # Update forward index in page title 
        all_title_tokens = title_tokens_clean.copy()
        all_title_tokens.extend(tokenize_bigram(title_tokens_clean))
        all_title_tokens_ID = [title_word_to_wordID[word] for word in all_title_tokens]
        title_forward_index[currentPageID] = all_title_tokens_ID
        
        # Update inverted index in page title
        UpdateTitleInvertedIndex(title_tokens_clean, currentPageID)
        UpdateTitleInvertedIndex(tokenize_bigram(title_tokens_clean), currentPageID)


        # pageID to details
        pageID_details[currentPageID] = [page_title, last_modified_date, len(str(soup.contents)), {}, []]
        pageID_details[currentPageID][3] = word_freq_combine    
        

        # parent child linkage for insertion into page details
       
        ## append child page if not included yet in parent details
        for parent_pageID in parent_ID:
            if parent_pageID != -1:
                if currentPageID not in pageID_details[parent_pageID][4]:
                    pageID_details[parent_pageID][4].append(currentPageID)

        if save_html:
            SaveHTML(currentPageID, soup)

        # find all child pages
        child_urls = soup.findAll("a", href=True)
        for links in child_urls:
            full_url = urljoin(url, links["href"]).rstrip('/')

            # identify parent child linkage for visited child page for current page
            if full_url in url_to_pageID.keys():
                if url_to_pageID[full_url] not in pageID_details[currentPageID][4]:
                    pageID_details[currentPageID][4].append(url_to_pageID[full_url])

            # pre-identify parent child linkage in the queue
            queue_first = [q[0] for q in queue]
            ##print(queue) # for checking
            if full_url in queue_first:
                idx = queue_first.index(full_url)
                if currentPageID not in queue[idx][1]:
                    queue[idx][1].append(currentPageID)
            
            # check whether url is visited or in queue
            if  (full_url in url_to_pageID.keys()) or (full_url in queue_first):
                continue
            else:
                queue.append([full_url, [currentPageID]])
   

    # crawl page by drawing from the queue iteratively
        if len(queue)>0:
          current = queue.popleft()
          crawl(current[0], current[1])


def AssignParent(currentID):
    parent_list = []
    for parentID, details in pageID_details.items():
        if int(currentID) in details[4]:
          parent_list.append(parentID)
    return parent_list


def SaveSqliteDict(_dict: collections.OrderedDict, _directory):
    sqliteDict = SqliteDict(_directory, autocommit=True)
    sqliteDict = SqliteDict(_directory)
    print("Saving database to" + _directory)
    i = 0
    for key, value in _dict.items():
        i += 1
        sqliteDict[key] = value
        if i %1000 ==0:
          print("progress:", _directory, str(i), "out of", str(len(_dict) - 1))
    sqliteDict.commit()
    sqliteDict.close()

# to crawl pages
crawl(url, [-1])

# to assign parent child linkage

for pageID in pageID_details.keys():
  parent = AssignParent(pageID)
  parent_no_cyclic = []
  for i in range(len(parent)):
    if int(parent[i]) < int(pageID): #handle cyclic linkages
      parent_no_cyclic.append(int(parent[i]))
  pageID_to_parentID[pageID] = parent_no_cyclic

save_to_db = True
if save_to_db:
    SaveSqliteDict(url_to_pageID, './app/db/url_to_pageID.sqlite')
    SaveSqliteDict(pageID_to_url, './app/db/pageID_to_url.sqlite')

    SaveSqliteDict(body_word_to_wordID, './app/db/body_word_to_wordID.sqlite')
    SaveSqliteDict(body_wordID_to_word, './app/db/body_wordID_to_word.sqlite')
    SaveSqliteDict(body_forward_index, './app/db/body_forward_index.sqlite')
    SaveSqliteDict(body_inverted_index, './app/db/body_inverted_index.sqlite')

    SaveSqliteDict(title_word_to_wordID, './app/db/title_word_to_wordID.sqlite')
    SaveSqliteDict(title_wordID_to_word, './app/db/title_wordID_to_word.sqlite')
    SaveSqliteDict(title_forward_index, './app/db/title_forward_index.sqlite')
    SaveSqliteDict(title_inverted_index, './app/db/title_inverted_index.sqlite')    

    SaveSqliteDict(pageID_details, './app/db/pageID_details.sqlite')
    SaveSqliteDict(pageID_to_parentID, './app/db/pageID_to_parentID.sqlite')

#sys.exit()

